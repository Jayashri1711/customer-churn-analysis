# -*- coding: utf-8 -*-
"""Customer Churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYYhXG8Aoa6uS4AXSZmTnHkoyosmLwPS

Importing the dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import pickle

"""2.Data Loading and Understanding"""

df=pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")

df.shape

df.head()

pd.set_option("display.max_columns",None)

df.head(2)

df.info()

#dropping customerID column as this is not required for modeling
df=df.drop(columns=["customerID"])

df.head(2)

#printing the unique values in all the columns
numerical_features_list=["tenure","MonthlyCharges","TotalCharges"]
for col in df.columns:
  if col not in numerical_features_list:
    print(col,df[col].unique())
    print("-"*50)

print(df.isnull().sum())

#df["TotalCharges"]=df["TotalCharges"].astype(float)

df[df["TotalCharges"]==" "]

len(df[df["TotalCharges"]==" "])

df["TotalCharges"]=df["TotalCharges"].replace({" ":"0.0"})

df["TotalCharges"] = df["TotalCharges"].replace(" ", np.nan)
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")
df["TotalCharges"].fillna(df["TotalCharges"].median(), inplace=True)  # Fill NaNs with median

df.info()

#checking the class distribution of target column
print(df["Churn"].value_counts())

"""**Insights**:

1.Customer ID is removed as it is not required for modeling

2.No missing values in the dataset

3.Missing values in the TotalCharges column were replaced with 0

4.Class imbalance identified in the target

3.Exploratory Data Analysis(EDA)
"""

df.shape

df.columns

df.head(2)

df.describe()

"""Numerical Features-Analysis

Understand the distribution of the numerical feautures
"""

def plot_histogram(df,column_name):

  plt.figure(figsize=(5,3))
  sns.histplot(df[column_name],kde=True)
  plt.title(f"Distribution of {column_name}")

  #calculate the mean and median values for the columns
  col_mean=df[column_name].mean()
  col_median=df[column_name].median()

  #Add vertical lines for mean and median
  plt.axvline(col_mean,color="red",linestyle="--",label="Mean")
  plt.axvline(col_median,color="green",linestyle="-",label="Median")

  plt.legend()

  plt.show()

plot_histogram(df,"tenure")

plot_histogram(df,"MonthlyCharges")

plot_histogram(df,"TotalCharges")

"""Box plot for numerical features"""

def plot_boxplot(df,column_name):
  plt.figure(figsize=(5,3))
  sns.boxplot(y=df[column_name])
  plt.title(f"Box Plot of {column_name}")
  plt.ylabel("column_name")
  plt.show

plot_boxplot(df,"tenure")

plot_boxplot(df,"MonthlyCharges")

plot_boxplot(df,"TotalCharges")

"""Correlation Heatmap for numerical Columns"""

#correlation matrix-heapmap
plt.figure(figsize=(8,4))
sns.heatmap(df[["tenure","MonthlyCharges","TotalCharges"]].corr(),annot=True,cmap="coolwarm",fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""Categorical features-Analysis"""

df.columns

df.info()

"""Countplot for categorical columns"""

object_cols=df.select_dtypes(include="object").columns.to_list()
object_cols=["SeniorCitizen"]+object_cols

for col in object_cols:
  plt.figure(figsize=(5,3))
  sns.countplot(x=df[col])
  plt.title(f"Count Plot of {col}")
  plt.show()

"""4.Data Preprocessing"""

df.head(3)

"""Label encoding of target column"""

df["Churn"]=df["Churn"].replace({"Yes":1,"No":0})

df.head(2)

print(df["Churn"].value_counts())

"""Label encoding of categorical features"""

#identifying columns with object data type
object_columns=df.select_dtypes(include="object").columns

print(object_columns)

#initial a dictionary to save the encoders
encoders={}

#apply label encoding and stire the encoders
for column in object_columns:
  label_encoder=LabelEncoder()
  df[column]=label_encoder.fit_transform(df[column])
  encoders[column]=label_encoder

#save the encoders to a pickle file
with open("encoders.pkl","wb") as f:
  pickle.dump(encoders,f)

encoders

df.head()

"""Training and test data split"""

#splitting the features and target
X=df.drop(columns=["Churn"])
y=df["Churn"]

#split training and test data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

print(y_train.shape)

print(y_train.value_counts())

"""Synthetic Minority Oversampline technique (SMOTE)"""

smote=SMOTE(random_state=42)

X_train_smote,y_train_smote=smote.fit_resample(X_train,y_train)

print(y_train_smote.shape)

print(y_train_smote.value_counts())

"""**5.Model Training**

Training with default hyperparameters
"""

#dictionary of models
models={
    "Decision Tree":DecisionTreeClassifier(random_state=42),
    "Random forest":RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42)
}

# dictionary to store the cross validation results
cv_scores={}

#perform 5-fold cross validation for each model
for model_name,model in models.items():
  print(f"Training {model_name} with default parameters")
  scores=cross_val_score(model,X_train_smote,y_train_smote,cv=5,scoring="accuracy")
  cv_scores[model_name]=scores
  print(f"{model_name} cross-validation accuracy:{np.mean(scores):.2f}")
  print("-"*70)

cv_scores

"""Random Forest gives the highest accuracy compared to other models with default parameters"""

rfc=RandomForestClassifier(random_state=42)

rfc.fit(X_train_smote,y_train_smote)

print(y_test.value_counts())

"""6.Model Evaluation"""

#evaluate on test data
y_test_pred=rfc.predict(X_test)
print("Accuracy Score:\n",accuracy_score(y_test,y_test_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_test_pred))
print("Classification Report:\n",classification_report(y_test,y_test_pred))

#save the trained model as a pickle file
model_data={"model":rfc,"features_names":X.columns.tolist()}
with open("customer_churn_model.pkl","wb") as f:
  pickle.dump(model_data,f)

"""7.Load the same model and build a predictive system"""

#Load the saved model and the feature names

with open("customer_churn_model.pkl","rb") as f:
  model_data=pickle.load(f)

loaded_model=model_data["model"]
feature_names=model_data["features_names"]

print(loaded_model)

print(feature_names)

# Sample input data (excluding "customerID")
input_data = {
    'gender': 'Female',
    'SeniorCitizen': 0,
    'Partner': 'Yes',
    'Dependents': 'No',
    'tenure': 1,
    'PhoneService': 'No',
    'MultipleLines': 'No phone service',
    'InternetService': 'DSL',
    'OnlineSecurity': 'No',
    'OnlineBackup': 'Yes',
    'DeviceProtection': 'No',
    'TechSupport': 'No',
    'StreamingTV': 'No',
    'StreamingMovies': 'No',
    'Contract': 'Month-to-month',
    'PaperlessBilling': 'Yes',
    'PaymentMethod': 'Electronic check',
    'MonthlyCharges': 29.85,
    'TotalCharges': 29.85
}

# Convert input data to DataFrame
input_data_df = pd.DataFrame([input_data])

# Load encoders
try:
    with open("encoders.pkl", "rb") as f:
        encoders = pickle.load(f)
except FileNotFoundError:
    print("Error: encoders.pkl not found. Ensure the file is available.")
    exit()

# Ensure "customerID" is not being transformed
encoders = {k: v for k, v in encoders.items() if k in input_data_df.columns}

# Encode categorical variables safely
for column, encoder in encoders.items():
    if column in input_data_df.columns:
        try:
            input_data_df[column] = input_data_df[column].apply(
                lambda x: encoder.transform([x])[0] if x in encoder.classes_ else np.nan
            )
        except ValueError as e:
            print(f"Encoding error in column '{column}': {e}")

# Handle NaN values caused by unseen labels
for column in input_data_df.columns:
    if input_data_df[column].isna().sum() > 0:
        most_frequent = input_data_df[column].mode()[0]  # Replace NaN with most frequent value
        input_data_df[column].fillna(most_frequent, inplace=True)

# Load the trained model
try:
    with open("trained_model.pkl", "rb") as f:
        loaded_model = pickle.load(f)
except FileNotFoundError:
    print("Error: trained_model.pkl not found.")
    exit()

# Align input features with the model's training features
expected_features = loaded_model.feature_names_in_  # Get expected feature names
missing_features = [f for f in expected_features if f not in input_data_df.columns]

# Add missing features as NaN (or fill with zeros if needed)
for feature in missing_features:
    input_data_df[feature] = 0  # You can use np.nan if required

# Ensure the column order matches the model's training data
input_data_df = input_data_df[expected_features]

# Make a prediction
prediction = loaded_model.predict(input_data_df)
pred_prob = loaded_model.predict_proba(input_data_df)

# Results
print(f"Prediction: {'Churn' if prediction[0] == 1 else 'No churn'}")
print(f"Prediction Probability: {pred_prob}")

print(input_data_df.head())

"""To do:
   1.Implement Hyperparameter Tuining
   2.Try Model Selection
   3.Try downsampling
   4.Try to address the overfitting
   5.Try Startified k fold CV
"""